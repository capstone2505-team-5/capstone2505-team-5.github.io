---
// Implementation.astro - Case Study Implementation Section
import Carousel from '../Carousel.astro';
const workflowImages: {
  src: string;
  alt: string;
  caption: string;
  type: 'image' | 'video';
}[] = [
  {
    src: '/images/WorkflowCarousel/1-SelectProjectNarrow.png',
    alt: 'Select Project',
    caption: '1. Choose a Phoenix project to get started analyzing.',
    type: 'image'
  },
  {
    src: '/images/WorkflowCarousel/2-CreateBatchScreenshotFODMAP.png',
    alt: 'Create Batch',
    caption: '2. Filter, sort, and select the root spans you want to review.',
    type: 'image'
  },
  {
    src: '/images/WorkflowCarousel/3-FastManualReview-10s-edit.mp4',
    alt: 'Fast Manual Review',
    caption: '3. Rate with a binary score and add freeform notes to describe any errors found.',
    type: 'video'
  },
  {
    src: '/images/WorkflowCarousel/4-CategorizeInspectFast-10s-edit.mp4',
    alt: 'Categorize and Inspect',
    caption: '4. An LLM call automatically turns freeform notes into failure mode categories.',
    type: 'video'
  },
  {
    src: '/images/WorkflowCarousel/5-InspectMutlipleBatch.png',
    alt: 'Inspect Multiple Batch',
    caption: '5. Inspect multiple batches at once and view all categories across batches.',
    type: 'image'
  }
  
]

const reviewImages: {
  src: string;
  alt: string;
  caption: string;
  type: 'image' | 'video';
}[] = [
  {
    src: '/images/ReviewCarousel/HotkeyNavHighQuality.mp4',
    alt: 'Hotkey Nav',
    caption: 'Hotkeys streamline reviewing and navigation between spans, making the review process faster and more efficient. The review page automatically focuses the cursor on the freeform notes section, allowing reviewers to begin typing immediately without extra clicks. This design speeds up annotation, helping users maintain focus and increases the likelihood of grading an entire batch.',
    type: 'video'
  },
  {
    src: '/images/ReviewCarousel/InputOutputFormatHighQuality.mp4',
    alt: 'Input Output Format',
    caption: 'Inputs and outputs from an LLM are often in plain text or markdown, which can be tedious or difficult to review. LLMonade preserves the original content but automatically formats its display—using context specific layouts generated by an LLM. Whether it’s an email, a recipe, or another type of content, the information is shown in a clean, readable way that matches its intended structure.',
    type: 'video'
  },
  {
    src: '/images/ReviewCarousel/pdfContextHighQuality.mp4',
    alt: 'PDF Context',
    caption: 'Reviewers can attach PDFs or images (such as a FODMAP ingredients guide when evaluating recipe outputs) to the annotation panel. This reference persists across the entire batch, can be replaced at any time, and can be hidden when not needed—maximizing screen space for review of existing span data.',
    type: 'video'
  },
  {
    src: '/images/ReviewCarousel/PhoenixTraceContextHighQuality.mp4',
    alt: 'Phoenix Trace Context',
    caption: 'Each root span displayed in LLMonade includes a hyperlink back to its source trace data in Phoenix. This allows reviewers to explore the full trace and all nested spans when it is required to dive deeper into the additional spans within a trace.',
    type: 'video'
  },
  
]

---

<div class="case-study-content">
  <h3 id="introducing-llmonade">Introducing LLMonade</h3>
  <p>LLMonade is a free, lightweight, self-hosted platform that guides developers of LLM-powered applications through their first evaluations.  Our software is designed for small engineering teams that have already instrumented their application with Phoenix.  Traces are being collected, but the developers don’t know where to start assessing and improving the quality of their application.  LLMonade pulls traces from Phoenix and provides an annotation interface inspired by Hamel Husain’s recommendations.  Then, guides the developer through the most important evaluation activity, manual error analysis.</p>


  <div class="metrics-grid">
    <div class="metric-card">
      <div class="metric-icon">
        <img src="/icons/cli-icon.png" alt="CLI Icon" />
      </div>
      <div class="metric-label">Deployment Command Line Interface (CLI)</div>
      <div class="metric-description">Quickly deploys required infrastructure to AWS.</div>
    </div>
    
    <div class="metric-card">
      <div class="metric-icon">
        <img src="/icons/etl-icon.png" alt="ETL Icon" />
      </div>
      <div class="metric-label">Root Spans Extract, Transform, Load Pipeline (ETL)</div>
      <div class="metric-description">Automatically ingests root spans from Phoenix.</div>
    </div>
    
    <div class="metric-card">
      <div class="metric-icon">
        <img src="/icons/app-icon.svg" alt="App Icon" />
      </div>
      <div class="metric-label">Fullstack Error Analysis Web Application</div>
      <div class="metric-description">Guides error analysis on an ideal review interface</div>
    </div>
  </div>

  <h4>Deployment Command Line Interface</h4>
  <p>The deployment Command Line Interface (CLI) is a tool that automatically spins up all required infrastructure to Amazon Web Services. It utilizes AWS Cloud Development Kit (CDK) to provision and connect the entire architecture without any manual configuration. LLMonade is an ideal solution for small teams to get started quickly with a self-hosted, AI evaluation workflow.</p>
  
  <h4>Root Spans Extract, Transform, Load Pipeline</h4>
  <p>Looking at real application data is the foundation of manual error analysis.  LLMonade integrates seamlessly with the AI observability tool Phoenix.  Once deployed, the ETL pipeline automatically ingests existing traces from Phoenix and any new traces that get generated. The highest level spans, the root spans, are formatted and saved to a database. This guarantees that the latest application behavior is always available for error analysis and review in the web application.</p>


  <h3 id="fullstack-error-analysis-web-application">Fullstack Error Analysis Web Application</h3>
  <p>At the heart of LLMonade is a full-stack web application.  The application features a simplified error analysis workflow that walks developers through their first manual evaluations.  The error analysis is accomplished through an optimized human review interface. This facilitates the viewing, annotating, and analysis of their LLM-powered application outputs.</p>
  <Carousel media={workflowImages} title="Error Analysis Workflow" id="workflow"/>
  <p>LLMonade walks users through a simplified five step workflow to quickly get started evaluating their application. This simplified workflow reflects Hamel Husain’s best practices for error analysis.  The inspection results provide data driven targets for developers to focus on.  Progress through the workflow is visually tracked in a persistent footer. It highlights the current error analysis step and clearly indicates what comes next, ensuring users new to evaluations can navigate the process with ease. </p>
  <h4>Effective Human Review Interface</h4>
  <p>Human review of traces is essential for identifying the most common failure modes in an application. However, manually reviewing root spans can be tedious, creating a barrier to starting this critical process. LLMonade removes this barrier with an efficient platform for faster and easier annotation and analysis.</p>
  <Carousel media={reviewImages} title="Effective Review Interface" id="review"/>


</div>


