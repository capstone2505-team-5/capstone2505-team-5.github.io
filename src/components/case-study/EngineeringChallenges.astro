---
// Results.astro - Case Study Results Section
import Image from "../Image.astro";
---

<div class="case-study-content">
  <h3 id="collecting-root-spans">Collecting Root Spans</h3>
  <h4>The Challenge</h4>
  <p>We needed a reliable, automated way to collect root spans from a user’s LLM application. The collection method had to work across diverse environments, minimize integration complexity, and support both current and historical trace data for evaluation.</p>

  <h4>Options</h4>
  <p>We evaluated several approaches to ingestion, each with distinct advantages and drawbacks. We considered creating our own SDK that users would integrate into their applications to send traces directly to LLMonade. This would provide us with direct trace collection and full control over the ingestion process.  However, real time ingestion would not retrieve historical traces. This is a critical shortcoming for teams needing to analyze applications already running in production. Lastly, building efficient trace collection infrastructure is complex: it must handle high trace volumes while maintaining zero performance impact on user applications.</p>
  
  <h4>Our Solution</h4>
  <p>We chose to leverage the proven observability tool Phoenix to collect trace data. Phoenix is open source with no licensing cost or vendor lock-in, supports self-deployment in user environments, and provides well-documented GraphQL endpoints. These features align with LLMonade’s own goals of supporting small teams who want to maintain ownership over their data.</p>

  <p>Many AI teams already use Phoenix for observability. This allows us to integrate into existing workflows and create a consistent, dependable ecosystem for LLM observability and evaluation without building our own tracing infrastructure. We view our application as a stepping stone for AI application developers. Retaining trace data in Phoenix benefits our users as their applications scale to serve more users.</p>

  <Image 
  src="/images/_phoenix-vs-sdk-edit.png" 
  alt="Phoenix vs SDK" 
  caption="Phoenix vs SDK"
  width="600px"
  height="325px"
  bordered={true}
  rounded={true}
  clickable={false}/>
  <h4>Tradeoffs</h4>
  <p>The main issue with choosing Phoenix as our trace provider is that it becomes a single point of failure for our application. Pulling Phoenix trace data and storing it in our own database helps to mitigate this dependency. By having redundant data, our users can still evaluate root spans even if Phoenix’s API is down. While relying on Phoenix to provide traces simplifies many of the trace collection challenges, ingesting data from it to populate LLMonade’s databases still was not a trivial task. </p>

  <h3 id="data-ingestion">Data Ingestion</h3>
  <h4>The Challenge</h4>
  <p>Adopting Phoenix for observability introduced two ingestion challenges.  First, we needed to perform a high-volume backfill of potentially tens of thousands of historical traces. Second, we had to establish a continuous pipeline to retrieve newly generated traces. </p>
  
  <h4>Options</h4>
  <p>One approach we considered was directly streaming from the user’s Phoenix database using a pub/sub architecture. This approach would allow for near real-time data streaming, however, it requires users to have Phoenix deployed within the same AWS architecture as our application.  Also, it would tightly couple LLMonade with internal implementation details of Phoenix, complicating future efforts to integrate with other observability tools. Lastly, it presents security concerns, as users might be hesitant to grant direct database access to our application.</p>
  <p>We also considered allowing users to bulk upload traces via CSV or JSON import files. This would give them complete control over what trace data they would like to evaluate and remove the direct dependency on Phoenix for observability. The downside is that manually exporting traces and manually importing traces would have to be done every time the user has new data.  Our target demographic most likely wants to review their recent traces and thus would have to export and import data every time they use the application.</p>
  <p>Finally, Phoenix offers a GraphQL API. The API is an officially supported, performant mechanism for fetching traces.  This method can be automated while also keeping our application decoupled from the Phoenix database.</p>
  
  <h4>Our Solution</h4>
  <p>We decided to use Phoenix’s GraphQL API to handle all data ingestion. When our AWS architecture is initially deployed, a Lambda function is triggered to retrieve all available projects from the user’s Phoenix account. For each project, an instance of a separate Lambda function is created to retrieve all traces specific to that project.  These Lambda instances operate in parallel, fetching batches of 2,000 traces and processing up to 100,000 total traces per invocation.  This prevents any single project from creating a bottleneck in the ingestion process.  We also configured the function's Reserved Concurrency to prevent our database from being overwhelmed with connections.</p>
  <p>In order to keep trace data up to date, we trigger the same two-Lambda system every five minutes via AWS Eventbridge scheduling. This polling frequency can be easily configured based on the specific needs of a development team. Our Lambda-based approach with Eventbridge creates a completely serverless ETL pipeline with no infrastructure maintenance.  It delivers a lightweight and cost-effective ingestion solution which accomplishes near real-time visibility of Phoenix trace data.</p>
  
  <h4>Tradeoffs</h4>
  <p>One consideration is that the speed of our data collection is subject to API rate limiting from Phoenix. We determined this was an acceptable constraint for our target audience of small teams. The rate limit is generous enough that users are very unlikely to encounter it during normal operation. Reaching the limit would require an ingestion volume of tens of thousands of traces per minute.</p>
  <p>It is also important to note that our pull-based model leads to some inefficient API calls. The system periodically polls for new information which means it sometimes makes requests when no new data exists. We accepted this behavior because the API requests are free and have a low performance impact.</p>

  <h3 id="flexible-evaluation-platform">Flexible Evaluation Platform</h3>
  <h4>The Challenge</h4>
  <p>We endeavored to create an evaluation tool that works immediately for common scenarios without requiring extensive setup or customization. There are many different types of LLM-powered applications, each with varying complexity and data flows. A simple chatbot involves just a query-response pair, while a RAG system might have multiple retrieval steps, an agentic workflow involves tool calls and reasoning steps.  </p>
  <p>Furthermore, depending on the goal of the user’s application, the output could have many different content types. Outputs could be emails, code snippets, structured data, conversational text, cooking recipes, or anything else an LLM can generate. </p>
  <p>In manual error analysis, the reviewer often needs context beyond the input and output.  For example, to review an LLM-app that drafts emails for job applicants, the reviewer may need to look at the actual job posting.  We needed a uniform way to display the context, even though it could come in many different shapes, sizes, and file types.</p>
  
  <h4>Our Solution</h4>
  <p>To handle the varied types of LLM-powered applications we extracted the root span from traces.  The root span represents the primary input and final output of a trace. This technique bypasses the complexity of multi-step LLM chains. It allows the evaluation to focus on the fundamental relationship between what the user asks for and what the application delivers.</p>
  <p>LLMonade incorporates an AI feature for automatic content type detection. The system uses AI to automatically identify outputs, like an email or a recipe, and then formats the data for a consistent presentation. Domain experts can review the inputs and outputs in a similar format to how they are meant to be displayed.</p>
  <p>We recognized that domain experts often need additional context to make accurate assessments. To support this, we implemented a flexible context window where users can drag and drop PDF documents and images. This feature allows evaluators to view domain specific requirements and nuances directly alongside the inputs and outputs. The entire review process is presented together in a single, unified view.</p>

  <h4>Tradeoffs</h4>
  <p>Our standardized, flexible approach involves important compromises that balance simplicity with effectiveness. By focusing on just the root spans, we intentionally hide the internal complexity of multi-step LLM processes.  We simplify the annotation screen and thus speed up the process for the reviewer.  For users who need deeper insight into their application’s inner workings, we provide direct links to the full trace data in Phoenix. This allows them to examine complete process details with a single click when necessary.</p>
  <p>Additionally, our AI-powered content detection and formatting can take up to a minute to process for large batches of traces.  We recognized that this could slow down the review process.  Thus, we circumvent this issue by initially showing users the raw unformatted content while an LLM formats the root spans in the background asynchronously. Once they’re ready, the formatted root spans automatically appear in the grading page. This approach prioritizes immediate user access to data while providing enhanced formatting when available. </p>
  <p>Our implementation balances user experience with resource efficiency by calling the LLM’s API only when users actually view specific root spans. This avoids unnecessarily formatting data that may never be reviewed. Rather than pre-formatting all ingested root spans, we format on-demand.  This significantly reduces both API usage and associated costs, while delivering a smooth evaluation experience.</p>


  <h3 id="serverless-architecture">Serverless Architecture</h3>
  <h4>The Challenge</h4>
  <p>As a tool for small development teams, LLMonade needed to be highly cost-effective, especially for intermittent usage patterns. The system had to scale automatically if there were bursts of user traffic.  It also must be secure because the trace data it handles often contains sensitive information. A final goal was to eliminate the operational burden of server provisioning and maintenance.</p>

  <h4>Options</h4>
  <p>We first considered a traditional Elastic Compute Cloud (EC2) based architecture, which offers the advantages of granular control and no cold start latency. This approach would introduce the operational overhead of server management and capacity planning, as well as the higher baseline costs of maintaining idle infrastructure.</p>
  <p>We also evaluated container-based solutions like AWS Elastic Container Service (ECS) on EC2 and Fargate. These solutions offered better resource utilization than standalone EC2 instances but introduced more operational complexity and higher costs than a serverless architecture. Given our expected variable and low-frequency workloads, using Lambda for the backend was the most cost effective solution.</p>

  <h4>Our Solution</h4>
  <p>We implemented a serverless architecture using AWS Lambda functions paired with API gateway for backend processing. This is complemented by S3 and Cloudfront for frontend delivery. API Gateway handles authentication through AWS Cognito integrations.  This provides robust security for users’ sensitive trace data while maintaining ease of access for authorized users. This serverless approach delivers a true pay-per-use model that automatically scales to meet user demand.  The architecture eliminates the need for capacity planning, server provisioning, or infrastructure maintenance while ensuring high availability and performance.</p>
  <Image 
  src="/images/_serverless-comparison-edit.png" 
  alt="Serverless Architecture" 
  caption="Serverless Architecture"
  width="600px"
  height="325px"
  bordered={true}
  rounded={true}
  clickable={false}/>
  <h4>Tradeoffs</h4>
  <p>Lambda cold starts can introduce additional latency for the first request after periods of inactivity. However, for developers using an evaluation tool, a 2-second cold start is acceptable.  The analysis on LLMonade is done asynchronously from the customer facing AI product and thus does not need to be instant.  The slight delay is well worth the cost savings from a serverless architecture.</p>
  <p>The serverless model also limits our ability to maintain persistent state or long-running processes. This specifically applies to our AI-powered content formatting, which can take up to a minute. To accommodate this within Lambda's execution time limits, we designed an asynchronous workflow. It also requires the frontend to poll for status updates rather than maintaining a continuous connection. This adds some complexity to our frontend logic but keeps the backend architecture simple and cost-effective.</p>

</div>


