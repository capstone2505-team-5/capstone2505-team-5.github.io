---
// Implementation.astro - Case Study Implementation Section
import Image from "../Image.astro";
---

<div class="case-study-content">
  <h3 id="integrate-with-more-ai-observability-platforms">Integrate with More AI Observability Platforms</h3>
  <p>Currently, we only offer our evaluation to those who are using Phoenix as their observability platform. There are many other observability platforms such as Braintrust, Langfuse, and Langsmith that we could integrate with so that users aren’t locked in to Phoenix. Furthermore, in addition to automated trace collection, we would like to offer users the option to directly upload trace data through CSV or JSON files to have more direct management over what traces they want to evaluate.</p>

  <h3 id="expand-deployment-options">Expand Deployment Options</h3>
  <p>Because AI applications are so new, there are many developers who haven’t set up an observability tool yet. We could provide an option in our CLI to deploy Phoenix tracing infrastructure within the same AWS architecture as LLMonade - an all in one package to get setup with LLM observability and evaluation.</p>

  <p>Since LLM data, especially user inputs, can contain sensitive information, we would also like to offer deployment mechanisms that support different security constraints such as access through VPN.</p>

  <p>Finally, not every team uses AWS, so in the future we would like to transition from using AWS CDK and instead use Terraform to allow users to deploy to other cloud providers such as Microsoft Azure and Google Cloud Platform (GCP).</p>

  <Image src="/images/cloud-providers-light-gradient.png" alt="Cloud providers" caption="" clickable={true} />

  <h3 id="provide-more-app-functionality">Provide More App Functionality</h3>
  <p>Future development will focus on more comprehensive evaluation processes. We could enable viewing of entire traces across multiple spans, beyond our current root span focus, to give users a deeper insight into their LLM applications. </p>

  <p>Context enhancement also represents a major area for future improvement. Future iterations can incorporate richer metadata on traces, API calls to external services, and introduce moveable and resizeable interface components for a more customizable user experience.</p>

  <p>Finally, using synthetic datasets alongside real traces could enable experimental testing scenarios for applications still in development, allowing teams to evaluate their LLM systems before they have real user data. </p>

  <p>As companies scale and generate larger volumes of traces, manual evaluation becomes unfeasible so we could add LLM-as-a-judge functionality. This would enable automated evaluation scoring allowing our application to support teams as their evaluation workloads grow. These enhancements would allow users to start out with a simplified error analysis workflow and gradually transition to an automated evaluation platform with guidance at every step.</p>

</div>


